\chapter{决策树\label{dt}}
\section{The metrics for measuring a split}
\subsection{Gini Impurity}
\begin{tcolorbox}
    \begin{definition}[Gini不纯度]
        Gini Impurity, as its name implies, measures the impurity rate of the class distribution of data points, or the class mixture rate. For a dataset with K classes, suppose that data from class $k(1 \leq k \leq K)$ takes up a fraction $f_k(0 \leq f_k \leq 1)$ of the entire dataset; then the Gini Impurity of this dataset is written as follows:
        $$Gini~impurity=1-\sum_{k=1}^Kf_k^2$$
    \end{definition}
\end{tcolorbox}
A lower Gini Impurity indicates a purer dataset.

\subsection{Information Gain}
\begin{definition}
    Entropy is a probabilistic measure of uncertainty. Given a $K$-class dataset, and $f_k (0 \leq f_k \leq 1)$ denoted as the fraction of data from class $k (1 \leq k \leq K)$, the entropy of the dataset is defined as follows:
    $$Entropy = -\sum_{k=1}^Kf_k*\log_2f_k$$
\end{definition}

\begin{tcolorbox}
    \begin{definition}[Information Gain]
        Information Gain, measures the improvement of purity after splitting or, in other words, the reduction of uncertainty due to a split. Higher Information Gain implies better splitting. We obtain the Information Gain of a split by comparing the entropy before and after the split.
        \begin{equation*}
            \begin{aligned}
                Information~Gain & =Entropy(before)-Entropy(After)    \\
                                 & =Entropy(Parent)-Entropy(Children) \\
            \end{aligned}
        \end{equation*}
        Lower entropy implies a purer dataset with less ambiguity.
    \end{definition}
\end{tcolorbox}